{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 20 11:15:06 2019\n",
    "\n",
    "@author: eabernal\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "#from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %%\n",
    "#torch.manual_seed(1)    # reproducible\n",
    "# Hyper Parameters\n",
    "NEPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 28          # rnn time step / image height\n",
    "INPUT_SIZE = 28         # rnn input size / image width\n",
    "LR = 0.01               # learning rate\n",
    "DOWNLOAD_MNIST = False   # set to True if haven't downloaded the data\n",
    "\n",
    "# %%\n",
    "# Mnist dataset\n",
    "train_data = dsets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                         # this is training data\n",
    "    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                        # torch.FloatTensor of shape (C x H x W) and normalizes to range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,            # download it if you don't have it\n",
    ")\n",
    "\n",
    "# %% \n",
    "# display example\n",
    "print(train_data.train_data.size())     # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())   # (60000)\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[0])\n",
    "plt.show()\n",
    "\n",
    "# Data Loader for easy mini-batch return in training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# convert test data into Variable\n",
    "test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())\n",
    "test_x = test_data.test_data.float()/255.\n",
    "test_y = test_data.test_labels.numpy().squeeze()    # covert to numpy array\n",
    "\n",
    "# %%\n",
    "\n",
    "#class RNN(nn.Module):\n",
    "#    def __init__(self):\n",
    "#        super(RNN, self).__init__()\n",
    "#\n",
    "#        self.rnn = nn.LSTM(         \n",
    "#            input_size = INPUT_SIZE,\n",
    "#            hidden_size = 64,         # number of hidden units\n",
    "#            num_layers = 1,           # number of layers\n",
    "#            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you don’t need batch_first=True and your LSTM will give output of shape (seq_len, batch.\n",
    "#            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your LSTM will give output of shape (batch_size, seq_len, hidden_size).\n",
    "#        )\n",
    "#        self.out = nn.Linear(64, 10)\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        # x shape (batch, time_step, input_size)\n",
    "#        # r_out shape (batch, time_step, output_size)\n",
    "#        # h_n shape (n_layers, batch, hidden_size)\n",
    "#        # h_c shape (n_layers, batch, hidden_size)\n",
    "#        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "#\n",
    "#        # choose last time step of r_out\n",
    "#        out = self.out(r_out[:, -1, :])\n",
    "#        return out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(         \n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 64,         # number of hidden units\n",
    "            num_layers = 1,           # number of layers\n",
    "            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you don’t need batch_first=True and your RNN will output a tensor with shape (seq_len, batch.\n",
    "            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your RNN will output a tensor with shape (batch_size, seq_len, hidden_size).\n",
    "        )\n",
    "        self.out = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        # r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "        r_out, h = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose last time step of output\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hot\n",
    "\n",
    "# %% training and testing\n",
    "for epoch in range(NEPOCHS):\n",
    "    for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "        b_x = x.view(-1, 28, 28)              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = y                               # batch y\n",
    "\n",
    "        output = rnn(b_x)                               # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            test_output = rnn(test_x)                   # (samples, time_step, input_size)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y) / float(test_y.size)\n",
    "            print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
