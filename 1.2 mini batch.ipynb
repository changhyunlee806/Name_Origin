{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "\n",
    "Mini Batch Size\n",
    "\n",
    "Set batch_size to 1000, 2000 and 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def findName(dict, name):\n",
    "    keys = dict.keys()\n",
    "    for key in keys:\n",
    "        if name in dict[key]:\n",
    "            return key\n",
    "    return ''\n",
    "\n",
    "import torch\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples (language, name)\n",
    "\n",
    "listOfTuples = []\n",
    "dictNames = names\n",
    "count = 0\n",
    "def createTuple():\n",
    "    for lang, listname in dictNames.items():\n",
    "        for n in listname:\n",
    "            tupleLangName = (n, lang)\n",
    "            listOfTuples.append(tupleLangName)\n",
    "\n",
    "            \n",
    "createTuple()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to find max length name\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def maxlength(data):\n",
    "    index = 0\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i][0]) > len(data[index][0]):\n",
    "            index = i\n",
    "    return len(data[index][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getBatchOfData(batch_size, data):\n",
    "    num_batches = int(len(data)/batch_size)\n",
    "    indices = np.arange(0, len(data))\n",
    "    np.random.shuffle(indices)\n",
    "        \n",
    "    bx = []\n",
    "    by = []\n",
    "    batches = []\n",
    "    eachBatch = []\n",
    "    num_samples = 0\n",
    "    \n",
    "    for index1 in indices:\n",
    "        num_samples += 1\n",
    "        eachBatch.append(data[index1])\n",
    "       \n",
    "        if num_samples % batch_size == 0:\n",
    "            batches.append(eachBatch)\n",
    "            eachBatch = []\n",
    "\n",
    "    for i in range(len(batches)):\n",
    "        BatchOfnameTensor = torch.zeros(batch_size, maxlength(batches[i]), n_letters)\n",
    "        BatchOflanguageTensor = torch.zeros(batch_size, n_categories, dtype=torch.long)\n",
    "        for j in range(len(batches[i])):\n",
    "            name_tensor = nameToTensor(batches[i][j][0])\n",
    "            for k in range(len(name_tensor)):\n",
    "                BatchOfnameTensor[j][k] = name_tensor[k][0]\n",
    "                \n",
    "            category = batches[i][j][1]\n",
    "            category_tensor = torch.zeros(n_categories, dtype=torch.long)\n",
    "            category_tensor[languages.index(category)] = 1\n",
    "            BatchOflanguageTensor[j] = category_tensor\n",
    "            \n",
    "        bx.append(BatchOfnameTensor)\n",
    "        by.append(BatchOflanguageTensor)\n",
    "        \n",
    "    return bx, by\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize, nlayers, outputsize):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(         \n",
    "            input_size = inputsize,\n",
    "            hidden_size = hiddensize,         # number of hidden units\n",
    "            num_layers = nlayers,           # number of layers\n",
    "            batch_first = True       # If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your RNN will output a tensor with shape (seq_len, batch.\n",
    "        )\n",
    "        self.out = nn.Linear(128, n_categories)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train loss: 1.9302 | test accuracy: 0.453000\n",
      "Epoch:  2 | train loss: 1.8525 | test accuracy: 0.471000\n",
      "Epoch:  3 | train loss: 1.8495 | test accuracy: 0.456000\n",
      "Epoch:  4 | train loss: 1.8849 | test accuracy: 0.454000\n",
      "Epoch:  5 | train loss: 1.9058 | test accuracy: 0.457000\n",
      "0.457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "loss_func = nn.CrossEntropyLoss()                       \n",
    "\n",
    "epochs = 5\n",
    "batch_size = 1000 # Set to 1000, 2000, 5000\n",
    "\n",
    "# %% training and testing\n",
    "for epoch in range(1, epochs+1):\n",
    "        \n",
    "    n, l = getBatchOfData(batch_size, listOfTuples)\n",
    "   \n",
    "    \n",
    "    for batch in range(len(n)):\n",
    "        b_x = n[batch]\n",
    "        b_y = l[batch]\n",
    "        \n",
    "        output = rnn(b_x)                               # rnn output\n",
    "        loss = loss_func(output, torch.max(b_y, 1)[1])                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "    out = rnn(b_x)                   # (samples, time_step, input_size)\n",
    "    pred = torch.max(out, 1)[1].data.numpy().squeeze()\n",
    "    target = torch.max(b_y, 1)[1].data.numpy().squeeze()\n",
    "    accuracy = sum(pred == target) / b_y.size(0)\n",
    "\n",
    "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.6f' % accuracy)\n",
    "\n",
    "print(accuracy)       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
